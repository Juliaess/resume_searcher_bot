import re
import os
import time
import sqlite3
import logging
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Optional
import pdfplumber
import PyPDF2
from config import RESUMES_FOLDER
from utils import extract_name_from_filename
import aiosqlite
from cache_manager import cache_manager
from cachetools import LRUCache
import asyncio

logger = logging.getLogger(__name__)


class OptimizedPDFIndexer:
    def __init__(self, db_path: str = 'data/pdf_index.db', max_cache_size: int = 500):
        self.db_path = db_path
        self.search_semaphore = asyncio.Semaphore(5)
        self._pdf_texts_cache = LRUCache(maxsize=500)
        self.max_cache_size = max_cache_size
        self._lock = threading.Lock()
        self.init_index_database()
        self._setup_database_optimizations()

    async def optimize_database_indexes(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤"""
        async with aiosqlite.connect(self.db_path) as conn:
            cursor = await conn.cursor()

            await cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_fts_optimized 
                ON pdf_index_fts(content, candidate_name, filename)
            ''')

            await cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_access_stats 
                ON pdf_index(last_accessed, indexed_at)
            ''')

            await conn.commit()

    async def search_indexed_pdf_async(self, search_text: str, limit: int = 20):
        """ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º """
        cache_key = cache_manager.generate_key("pdf_search", search_text, limit)
        cached_results = await cache_manager.get(cache_key)
        if cached_results:
            logger.info(f"üì¶ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –∫—ç—à–∞ –¥–ª—è: {search_text[:50]}...")
            return cached_results

        async with self.search_semaphore:
            results = await self._perform_async_search(search_text, limit)
            await cache_manager.set(cache_key, results, ttl=3600)
            return results

    async def _perform_async_search(self, search_text: str, limit: int):
        """ –ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ """
        try:
            async with aiosqlite.connect(self.db_path) as conn:
                conn.row_factory = aiosqlite.Row
                cursor = await conn.cursor()

                logger.info(f"üîç –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫: '{search_text[:80]}...'")

                search_normalized = self._normalize_search_text(search_text)
                key_phrases = self._extract_search_phrases(search_normalized)

                if not key_phrases:
                    logger.warning("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ñ—Ä–∞–∑—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                    return await self._fallback_search_async(search_text, limit)

                results = []
                seen_filenames = set()

                for phrase in key_phrases[:10]:
                    phrase_results = await self._search_single_phrase_async(cursor, phrase, limit * 2)
                    for result in phrase_results:
                        if result['filename'] not in seen_filenames:
                            result['search_level'] = 'exact_phrase'
                            result['matched_phrase'] = phrase
                            results.append(result)
                            seen_filenames.add(result['filename'])

                if len(results) < 3:
                    combo_results = await self._search_by_word_combinations_async(cursor, key_phrases, limit)
                    for result in combo_results:
                        if result['filename'] not in seen_filenames:
                            result['search_level'] = 'word_combo'
                            results.append(result)
                            seen_filenames.add(result['filename'])

                final_results = []
                for result in results:
                    final_score = self._calculate_relevance(result, search_normalized, key_phrases)
                    result['relevance_score'] = final_score

                    if final_score >= 0.1:
                        final_results.append(result)

                final_results.sort(key=lambda x: x['relevance_score'], reverse=True)
                final_results = final_results[:limit]

                logger.info(f"‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ {len(final_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                return final_results

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return await self._fallback_search_async(search_text, limit)

    async def _search_single_phrase_async(self, cursor, phrase: str, limit: int) -> List[dict]:
        """ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑–µ """
        try:
            await cursor.execute('''
                   SELECT filename, candidate_name, content, 
                          snippet(pdf_index_fts, 2, '<b>', '</b>', '...', 64) as snippet
                   FROM pdf_index_fts 
                   WHERE pdf_index_fts MATCH ?
                   ORDER BY rank
                   LIMIT ?
               ''', (f'"{phrase}"', limit))

            rows = await cursor.fetchall()
            results = []
            for row in rows:
                results.append({
                    'filename': row['filename'],
                    'candidate_name': row['candidate_name'],
                    'file_path': os.path.join(RESUMES_FOLDER, row['filename']),
                    'relevance_score': 0.8,
                    'has_exact_match': True,
                    'content': row['content'],
                    'matched_phrase': phrase,
                    'snippet': row['snippet']
                })

            return results
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ FTS –ø–æ–∏—Å–∫–∞ —Ñ—Ä–∞–∑—ã '{phrase}': {e}")
            return []

    async def _search_by_word_combinations_async(self, cursor, phrases: List[str], limit: int) -> List[dict]:
        """ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º —Å–ª–æ–≤ """
        all_words = []
        for phrase in phrases:
            words = re.findall(r'\w{4,}', phrase.lower())
            all_words.extend(words)

        stop_words = {
            '–º–µ–Ω–µ–¥–∂–µ—Ä', '–ø—Ä–æ–¥–∞–∂–∞–º', '—Ä–∞–±–æ—Ç—ã', '–∫–ª–∏–µ–Ω—Ç–∞–º–∏', '–ø—Ä–æ–µ–∫—Ç', '–∫–æ–º–ø–∞–Ω–∏–∏',
            '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è', '—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '–∫–æ–Ω—Ç—Ä–æ–ª—å', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '—Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏–µ'
        }
        unique_words = [word for word in set(all_words) if word not in stop_words]

        if len(unique_words) < 2:
            return []

        logger.info(f"üîç –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º —Å–ª–æ–≤: {unique_words[:5]}")

        try:
            search_query = ' OR '.join(unique_words[:3])
            await cursor.execute('''
                   SELECT filename, candidate_name, content
                   FROM pdf_index_fts 
                   WHERE pdf_index_fts MATCH ?
                   ORDER BY rank
                   LIMIT ?
               ''', (search_query, limit * 2))

            rows = await cursor.fetchall()
            results = []
            for row in rows:
                content_lower = row['content'].lower()
                matched_count = sum(1 for word in unique_words[:3] if word in content_lower)

                if matched_count >= 2:
                    relevance = min(matched_count / len(unique_words[:3]), 0.6)
                    results.append({
                        'filename': row['filename'],
                        'candidate_name': row['candidate_name'],
                        'file_path': os.path.join(RESUMES_FOLDER, row['filename']),
                        'relevance_score': relevance,
                        'has_exact_match': False,
                        'matched_words': matched_count
                    })

            return results

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º —Å–ª–æ–≤: {e}")
            return []

    async def _fallback_search_async(self, search_text: str, limit: int = 20):
        """ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–∏—Å–∫ """
        try:
            async with aiosqlite.connect(self.db_path) as conn:
                conn.row_factory = aiosqlite.Row
                cursor = await conn.cursor()

                words = re.findall(r'\b\w{4,}\b', search_text.lower())
                stop_words = {'–æ–ø—ã—Ç', '—Ä–∞–±–æ—Ç—ã', '—Ä–∞–±–æ—Ç–∞', '–∫–æ–º–ø–∞–Ω–∏—è', '–ø—Ä–æ–µ–∫—Ç'}
                unique_words = [word for word in set(words) if word not in stop_words]

                if not unique_words:
                    unique_words = words[:3]

                logger.info(f"üîÑ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π fallback –ø–æ–∏—Å–∫ –ø–æ —Å–ª–æ–≤–∞–º: {unique_words}")

                all_results = []
                for word in unique_words[:3]:
                    await cursor.execute('''
                           SELECT filename, candidate_name, content
                           FROM pdf_index 
                           WHERE content LIKE ? 
                           LIMIT ?
                       ''', (f'%{word}%', limit))

                    rows = await cursor.fetchall()
                    for row in rows:
                        all_results.append({
                            'filename': row['filename'],
                            'candidate_name': row['candidate_name'],
                            'file_path': os.path.join(RESUMES_FOLDER, row['filename']),
                            'relevance_score': 0.3,
                            'has_exact_match': False,
                            'matched_word': word
                        })

                seen_files = set()
                final_results = []
                for result in all_results:
                    if result['filename'] not in seen_files:
                        seen_files.add(result['filename'])
                        final_results.append(result)

                final_results = final_results[:limit]
                logger.info(f"üîÑ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π fallback –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ {len(final_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                return final_results

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ fallback –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def _setup_database_optimizations(self):
        """ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SQLite –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("PRAGMA cache_size = -100000")
            cursor.execute("PRAGMA page_size = 4096")
            cursor.execute("PRAGMA mmap_size = 268435456")
            cursor.execute("PRAGMA temp_store = memory")
            cursor.execute("PRAGMA journal_mode = WAL")
            cursor.execute("PRAGMA synchronous = NORMAL")
            conn.commit()

    def init_index_database(self):
        """ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î """
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            cursor.execute("PRAGMA journal_mode=WAL")
            cursor.execute("PRAGMA synchronous=NORMAL")
            cursor.execute("PRAGMA cache_size=-10000")

            cursor.execute('''
                CREATE TABLE IF NOT EXISTS pdf_index (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    filename TEXT UNIQUE NOT NULL,
                    content TEXT,
                    candidate_name TEXT,
                    file_size INTEGER,
                    indexed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            cursor.execute('''
                        CREATE VIRTUAL TABLE IF NOT EXISTS pdf_index_fts 
                        USING fts5(
                            filename, 
                            content, 
                            candidate_name,
                            tokenize="porter unicode61"
                        )
                    ''')

            cursor.execute('CREATE INDEX IF NOT EXISTS idx_filename ON pdf_index(filename)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_candidate_name ON pdf_index(candidate_name)')

            conn.commit()

        logger.info("‚úÖ –ë–∞–∑–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    def index_all_pdfs(self, max_workers: int = 2, batch_size: int = 100):
        """ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è """
        pdf_files = [f for f in os.listdir(RESUMES_FOLDER) if f.lower().endswith('.pdf')]

        logger.info(f"üìö –ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {len(pdf_files)} PDF —Ñ–∞–π–ª–æ–≤...")

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT filename FROM pdf_index")
            existing_files = {row[0] for row in cursor.fetchall()}

        files_to_index = [f for f in pdf_files if f not in existing_files]

        if not files_to_index:
            logger.info("‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã")
            return len(pdf_files)

        logger.info(f"üìù –§–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {len(files_to_index)}")

        indexed_count = 0
        total_files = len(files_to_index)

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            batches = [files_to_index[i:i + batch_size]
                       for i in range(0, len(files_to_index), batch_size)]

            for batch_num, batch in enumerate(batches, 1):
                logger.info(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_num}/{len(batches)} ({len(batch)} —Ñ–∞–π–ª–æ–≤)")

                if batch_num > 1:
                    time.sleep(0.5)

                future_to_file = {
                    executor.submit(self._index_single_pdf, filename): filename
                    for filename in batch
                }

                batch_indexed = 0
                for future in as_completed(future_to_file):
                    filename = future_to_file[future]
                    try:
                        if future.result():
                            batch_indexed += 1
                            indexed_count += 1

                        if indexed_count % 50 == 0:
                            progress = (indexed_count / total_files) * 100
                            logger.info(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {indexed_count}/{total_files} ({progress:.1f}%)")

                    except Exception as e:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {filename}: {e}")

                logger.info(f"‚úÖ –ë–∞—Ç—á {batch_num}: –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {batch_indexed}/{len(batch)} —Ñ–∞–π–ª–æ–≤")

        logger.info(f"üéâ –ò—Ç–æ–≥: –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {indexed_count} —Ñ–∞–π–ª–æ–≤")
        return indexed_count

    def _index_single_pdf(self, filename: str) -> bool:
        """ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ PDF """
        max_retries = 3
        for attempt in range(max_retries):
            try:
                filepath = os.path.join(RESUMES_FOLDER, filename)
                if not os.path.exists(filepath):
                    return False

                text = self.extract_text_from_pdf(filepath, use_cache=False)
                if not text:
                    return False

                text_clean = self._clean_text(text[:20000])
                candidate_name = extract_name_from_filename(filename)
                file_size = os.path.getsize(filepath)

                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute('''
                        INSERT OR REPLACE INTO pdf_index 
                        (filename, content, candidate_name, file_size) 
                        VALUES (?, ?, ?, ?)
                    ''', (filename, text_clean, candidate_name, file_size))

                    cursor.execute('''
                                INSERT OR REPLACE INTO pdf_index_fts 
                                (filename, content, candidate_name) 
                                VALUES (?, ?, ?)
                            ''', (filename, text_clean, candidate_name))

                    conn.commit()
                    return True

            except sqlite3.OperationalError as e:
                if "database is locked" in str(e) and attempt < max_retries - 1:
                    logger.warning(f"‚ö†Ô∏è –ë–∞–∑–∞ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞, –ø–æ–≤—Ç–æ—Ä {attempt + 1}/{max_retries} –¥–ª—è {filename}")
                    time.sleep(0.2 * (attempt + 1))
                    continue
                else:
                    logger.error(f"‚ùå SQL –æ—à–∏–±–∫–∞ –¥–ª—è {filename}: {e}")
                    return False
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {filename}: {e}")
                return False

        return False

    def search_indexed_pdf(self, search_text: str, limit: int = 20):
        """ –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É """
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()

                logger.info(f"üîç –ü–æ–∏—Å–∫: '{search_text[:80]}...'")

                search_normalized = self._normalize_search_text(search_text)
                key_phrases = self._extract_search_phrases(search_normalized)

                if not key_phrases:
                    logger.warning("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ñ—Ä–∞–∑—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                    return self._fallback_search(search_text, limit)

                results = []
                seen_filenames = set()

                for phrase in key_phrases[:10]:
                    phrase_results = self._search_single_phrase(cursor, phrase, limit * 2)
                    for result in phrase_results:
                        if result['filename'] not in seen_filenames:
                            result['search_level'] = 'exact_phrase'
                            result['matched_phrase'] = phrase
                            results.append(result)
                            seen_filenames.add(result['filename'])

                if len(results) < 3:
                    combo_results = self._search_by_word_combinations(cursor, key_phrases, limit)
                    for result in combo_results:
                        if result['filename'] not in seen_filenames:
                            result['search_level'] = 'word_combo'
                            results.append(result)
                            seen_filenames.add(result['filename'])

                final_results = []
                for result in results:
                    final_score = self._calculate_relevance(result, search_normalized, key_phrases)
                    result['relevance_score'] = final_score

                    if final_score >= 0.1:
                        final_results.append(result)

                final_results.sort(key=lambda x: x['relevance_score'], reverse=True)
                final_results = final_results[:limit]

                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(final_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                return final_results

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return self._fallback_search(search_text, limit)

    def _search_single_phrase(self, cursor, phrase: str, limit: int) -> List[dict]:
        """ –ü–æ–∏—Å–∫ –ø–æ –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑–µ """
        try:
            cursor.execute('''
                SELECT filename, candidate_name, content, snippet(pdf_index_fts, 2, '<b>', '</b>', '...', 64) as snippet
                FROM pdf_index_fts 
                WHERE pdf_index_fts MATCH ?
                ORDER BY rank
                LIMIT ?
            ''', (f'"{phrase}"', limit))

            results = []
            for row in cursor.fetchall():
                results.append({
                    'filename': row['filename'],
                    'candidate_name': row['candidate_name'],
                    'file_path': os.path.join(RESUMES_FOLDER, row['filename']),
                    'relevance_score': 0.8,
                    'has_exact_match': True,
                    'content': row['content'],
                    'matched_phrase': phrase,
                    'snippet': row['snippet']
                })

            return results
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ FTS –ø–æ–∏—Å–∫–∞ —Ñ—Ä–∞–∑—ã '{phrase}': {e}")
            return []

    def _calculate_relevance(self, result: dict, search_text: str, key_phrases: List[str]) -> float:
        """ –†–∞—Å—á–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ """
        try:
            content = result.get('content', '').lower()
            if not content:
                return result['relevance_score']

            total_score = 0.0

            matched_phrases = [phrase for phrase in key_phrases if phrase.lower() in content]
            for phrase in matched_phrases[:3]:
                phrase_score = min(len(phrase) / 100, 0.5)
                total_score += phrase_score

            if len(matched_phrases) >= 2:
                total_score += 0.2
            elif len(matched_phrases) >= 1:
                total_score += 0.1

            return min(total_score, 1.0)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {e}")
            return result['relevance_score']

    def _extract_search_phrases(self, text: str) -> List[str]:
        """ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –¥–ª—è –ø–æ–∏—Å–∫–∞ """
        phrases = []

        list_patterns = [
            r'(?:\d+[\.\)]|\-|\*|\‚Ä¢|\‚Äî)\s*([^\n]{20,200})',
            r'\n\s*([^\n]{20,200})',  # —Å—Ç—Ä–æ–∫–∏ —Å –æ—Ç—Å—Ç—É–ø–æ–º
        ]

        for pattern in list_patterns:
            list_items = re.findall(pattern, text)
            phrases.extend([item.strip() for item in list_items if 20 <= len(item.strip()) <= 200])

        sentences = re.split(r'[.!?\n]', text)
        phrases.extend([s.strip() for s in sentences if 30 <= len(s.strip()) <= 250])

        companies = re.findall(r'(?:–∫–æ–º–ø–∞–Ω–∏[—è–∏—é]|–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏[—è–∏—é])\s+["¬´]?([^¬ª"\n]{10,100})', text, re.IGNORECASE)
        phrases.extend(companies)

        positions = re.findall(r'(?:—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞|–≤–Ω–µ–¥—Ä–µ–Ω–∏–µ|—Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏–µ)\s+([^\n.,!?]{15,150})', text, re.IGNORECASE)
        phrases.extend(positions)

        filtered_phrases = []
        seen_phrases = set()

        for phrase in phrases:
            if (phrase and phrase not in seen_phrases and 15 <= len(phrase) <= 250 and not self._is_too_general(phrase)):
                seen_phrases.add(phrase)
                filtered_phrases.append(phrase)

        filtered_phrases.sort(key=len, reverse=True)

        logger.info(f"üéØ –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ñ—Ä–∞–∑: {len(filtered_phrases)}")
        for i, phrase in enumerate(filtered_phrases[:5], 1):
            logger.info(f"   {i}. {phrase[:80]}...")

        return filtered_phrases[:15]

    def _is_too_general(self, phrase: str) -> bool:
        """ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã """
        general_phrases = {
            '–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã', '—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏', '–¥–æ–ª–∂–Ω–æ—Å—Ç–Ω—ã–µ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏',
            '–∫–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏', '–ª–∏—á–Ω—ã–µ –∫–∞—á–µ—Å—Ç–≤–∞',
            '–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '–Ω–∞–≤—ã–∫–∏', '—Ä–µ–∑—é–º–µ', '–∏—â—É —Ä–∞–±–æ—Ç—É', '—Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ',
            '–∑–∞–Ω—è—Ç–æ—Å—Ç—å', '–≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã', '–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –≤ –ø—É—Ç–∏'
        }

        phrase_lower = phrase.lower()
        return any(general in phrase_lower for general in general_phrases)

    def _search_by_word_combinations(self, cursor, phrases: List[str], limit: int) -> List[dict]:
        """ –ü–æ–∏—Å–∫ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º —Å–ª–æ–≤ """
        all_words = []
        for phrase in phrases:
            words = re.findall(r'\w{4,}', phrase.lower())
            all_words.extend(words)

        stop_words = {
            '–º–µ–Ω–µ–¥–∂–µ—Ä', '–ø—Ä–æ–¥–∞–∂–∞–º', '—Ä–∞–±–æ—Ç—ã', '–∫–ª–∏–µ–Ω—Ç–∞–º–∏', '–ø—Ä–æ–µ–∫—Ç', '–∫–æ–º–ø–∞–Ω–∏–∏',
            '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è', '—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '–∫–æ–Ω—Ç—Ä–æ–ª—å', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '—Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏–µ'
        }
        unique_words = [word for word in set(all_words) if word not in stop_words]

        if len(unique_words) < 2:
            return []

        logger.info(f"üîç –ü–æ–∏—Å–∫ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º —Å–ª–æ–≤: {unique_words[:5]}")

        try:
            search_query = ' OR '.join(unique_words[:3])
            cursor.execute('''
                        SELECT filename, candidate_name, content
                        FROM pdf_index_fts 
                        WHERE pdf_index_fts MATCH ?
                        ORDER BY rank
                        LIMIT ?
                    ''', (search_query, limit * 2))

            results = []
            for row in cursor.fetchall():
                content_lower = row['content'].lower()
                matched_count = sum(1 for word in unique_words[:3] if word in content_lower)

                if matched_count >= 2:
                    relevance = min(matched_count / len(unique_words[:3]), 0.6)
                    results.append({
                        'filename': row['filename'],
                        'candidate_name': row['candidate_name'],
                        'file_path': os.path.join(RESUMES_FOLDER, row['filename']),
                        'relevance_score': relevance,
                        'has_exact_match': False,
                        'matched_words': matched_count
                    })

            return results

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º —Å–ª–æ–≤: {e}")
            return []

    def _fallback_search(self, search_text: str, limit: int = 20):
        """ –†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–ª–æ–≤–∞–º """
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()

                words = re.findall(r'\b\w{4,}\b', search_text.lower())
                stop_words = {'–æ–ø—ã—Ç', '—Ä–∞–±–æ—Ç—ã', '—Ä–∞–±–æ—Ç–∞', '–∫–æ–º–ø–∞–Ω–∏—è', '–ø—Ä–æ–µ–∫—Ç'}
                unique_words = [word for word in set(words) if word not in stop_words]

                if not unique_words:
                    unique_words = words[:3]

                logger.info(f"üîÑ Fallback –ø–æ–∏—Å–∫ –ø–æ —Å–ª–æ–≤–∞–º: {unique_words}")

                all_results = []
                for word in unique_words[:3]:
                    cursor.execute('''
                        SELECT filename, candidate_name, content
                        FROM pdf_index 
                        WHERE content LIKE ? 
                        LIMIT ?
                    ''', (f'%{word}%', limit))

                    for row in cursor.fetchall():
                        all_results.append({
                            'filename': row['filename'],
                            'candidate_name': row['candidate_name'],
                            'file_path': os.path.join(RESUMES_FOLDER, row['filename']),
                            'relevance_score': 0.3,
                            'has_exact_match': False,
                            'matched_word': word
                        })

                seen_files = set()
                final_results = []
                for result in all_results:
                    if result['filename'] not in seen_files:
                        seen_files.add(result['filename'])
                        final_results.append(result)

                final_results = final_results[:limit]
                logger.info(f"üîÑ Fallback –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ {len(final_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                return final_results

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ fallback –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def _normalize_search_text(self, text: str) -> str:
        """ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ """
        if not text:
            return ""
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _clean_text(self, text: str) -> str:
        """ –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ """
        if not text:
            return ""
        text = ' '.join(text.split())
        return text[:20000]

    def _get_existing_filenames(self):
        """ –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT filename FROM pdf_index")
        existing_files = {row[0] for row in cursor.fetchall()}
        conn.close()
        return existing_files

    def get_index_stats(self):
        """ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM pdf_index")
            total_files = cursor.fetchone()[0]
            cursor.execute("SELECT COUNT(*) FROM pdf_index_fts")
            total_fts_files = cursor.fetchone()[0]
            cursor.execute("SELECT SUM(file_size) FROM pdf_index")
            total_size = cursor.fetchone()[0] or 0
            db_file_size = os.path.getsize(self.db_path) if os.path.exists(self.db_path) else 0

        return {
            'total_indexed_files': total_files,
            'total_fts_files': total_fts_files,
            'total_size_mb': total_size / (1024 * 1024),
            'db_size_mb': db_file_size / (1024 * 1024)
        }

    def cleanup_missing_files(self) -> int:
        """ –û—á–∏—Å—Ç–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ """
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                cursor.execute("SELECT filename FROM pdf_index")
                indexed_files = [row[0] for row in cursor.fetchall()]

                missing_files = []
                for filename in indexed_files:
                    filepath = os.path.join(RESUMES_FOLDER, filename)
                    if not os.path.exists(filepath):
                        missing_files.append(filename)

                if not missing_files:
                    logger.info("‚úÖ –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                    return 0

                batch_size = 100
                total_deleted = 0

                for i in range(0, len(missing_files), batch_size):
                    batch = missing_files[i:i + batch_size]
                    placeholders = ','.join('?' for _ in batch)

                    cursor.execute(
                        f"DELETE FROM pdf_index WHERE filename IN ({placeholders})",
                        batch
                    )

                    cursor.execute(
                        f"DELETE FROM pdf_index_fts WHERE filename IN ({placeholders})",
                        batch
                    )

                    deleted_count = cursor.rowcount
                    total_deleted += deleted_count
                    conn.commit()

                logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {total_deleted} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤")
                return total_deleted

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤: {e}")
            return 0

    def extract_text_from_pdf(self, pdf_path: str, use_cache: bool = True) -> Optional[str]:
        """ –ü–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF """
        cache_key = pdf_path

        if use_cache and cache_key in self._pdf_texts_cache:
            return self._pdf_texts_cache[cache_key]

        text = ""
        filename = os.path.basename(pdf_path)

        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è pdfplumber –Ω–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å {filename}, –ø—Ä–æ–±—É–µ–º PyPDF2: {e}")
            try:
                with open(pdf_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    for page_num in range(len(pdf_reader.pages)):
                        page = pdf_reader.pages[page_num]
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"

            except Exception as e2:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ {filename}: {e2}")
                return None

        result = text.strip() if text.strip() else None
        if use_cache and result:
            if len(self._pdf_texts_cache) >= self.max_cache_size:
                oldest_key = next(iter(self._pdf_texts_cache))
                del self._pdf_texts_cache[oldest_key]
            self._pdf_texts_cache[cache_key] = result

        return result

    def clear_cache(self):
        """ –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ """
        self._pdf_texts_cache.clear()
        logger.info("üßπ –ö—ç—à –æ—á–∏—â–µ–Ω")
        return True

    def optimize_database(self):
        """ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ """
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("PRAGMA optimize")
                cursor.execute("VACUUM")
                conn.commit()
            logger.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ë–î: {e}")
            return False


pdf_indexer = OptimizedPDFIndexer()